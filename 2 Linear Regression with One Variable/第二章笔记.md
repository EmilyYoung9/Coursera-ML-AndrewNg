# 2 **Linear Regression with One Variable**

> 2.1 
>
> 先介绍一下最基础的模型——单变量线性**回归**
>
> 2.2—2.4 
>
> 先介绍了一下 Hypothesis：$h_\theta(x) = \theta_0 + \theta_1 x$
>
> 要想让$h_\theta(x)$最好，就要让cost function最小，其中**平方误差代价函数**是解决回归问题最常用的手段
>
> Cost function：$J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$
>
> 2m是为了后续计算的方便
>
> 2.5—2.6
>
> 怎么样让代价函数$J(\theta_0, \theta_1)$最小，就考虑到了**梯度下降法**
>
> 2.7 
>
> 线性回归的梯度下降，因为$J(\theta_0, \theta_1)$是一个凸函数，所以没有局部最优解，只有全局最优解
>
> Batch梯度下降算法在每一步都用了训练集内所有的examples，以后还会学习其他的梯度下降算法，只用一个子集

## 2.1 模型描述 | Model representation

### 相关的表述Notation

①m 

②**x** 

③**y** 

④(x,y)

⑤$(x^{(i)},y^{(i)})$

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-1.png" alt="2-1" style="zoom:50%;" />

### 监督学习的大致流程

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-2.png" alt="2-2" style="zoom:50%;" />

h(hypothesis)只是一个规定叫法，可能并没有那么准确

先从最简单最基础的单变量线性回归学起

## 2.2 代价函数 | Cost function

### 代价函数的数学定义

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-3.png" alt="2-3" style="zoom:50%;" />

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-4.png" alt="2-4" style="zoom:50%;" />

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-5.png" alt="2-5" style="zoom:50%;" />

**平方误差代价函数**对于大多数问题，特别是回归问题，都是一个合理的选择

还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是**解决回归问题最常用的手段**

## 2.3 代价函数（一）| Cost function 1 

代价函数是用来做什么的？我们为什么要使用它？

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-6.png" alt="2-6" style="zoom:50%;" />

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-7.png" alt="2-7" style="zoom:50%;" />



## 2.4 代价函数（二）| Cost function 2 

### 基础回顾

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-8.png" alt="2-8" style="zoom:50%;" />

考虑有两个参数的情况下，代价函数的最小值

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-9.png" alt="2-9" style="zoom:50%;" />



<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-10.png" alt="2-10" style="zoom:50%;" />

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-11.png" alt="2-11" style="zoom:50%;" />

## 2.5 梯度下降法 | Gradient descent

梯度下降法不仅可以最小化线性回归的代价函数$J(\theta_0, \theta_1)$，还可以最小化其它代价函数

所以梯度下降法是一个基础的工具

### 用梯度下降算法最小化任意任意函数J

#### 问题概述

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-12.png" alt="2-12" style="zoom:50%;" />

#### 梯度下降法的一个有趣特点

| <img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-13.png" alt="2-13" style="zoom:50%;" /> | <img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-14.png" alt="2-14" style="zoom:50%;" /> |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

当你选择不同的起点，可能会得到不同的局部最优解

#### 背后的数学原理

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-15.png" alt="2-15" style="zoom:50%;" />

$\alpha$—learning rate，学习速率，决定了梯度下降的步子迈的有多大，决定我们以多大的幅度更新$\theta_0$和$\theta_1$

梯度下降法需要同时更新$\theta_0$和$\theta_1$

导数项：$\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)$

## 2.6 梯度下降知识点总结 | Gradient descent intuition

### 梯度下降算法

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-16.png" alt="2-16" style="zoom:50%;" />

### 一个稍微简单一点的例子：最小化函数只有一个参数的情形

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-17.png" alt="2-17" style="zoom:50%;" />

### $\alpha$的选择要适度

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-18.png" alt="2-18" style="zoom:50%;" />

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-19.png" alt="2-19" style="zoom:50%;" /><img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-20.png" alt="2-20" style="zoom:50%;" />

## 2.7 线性回归的梯度下降 | Gradient descent for linear regression

### 基础回顾（梯度下降法和线性回归模型）

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-21.png" alt="2-21" style="zoom:50%;" />

### 偏导数的推导过程

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-22.png" alt="2-22" style="zoom:50%;" />

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-23.png" alt="2-23" style="zoom:50%;" />

### 线性回归的$J(\theta_0, \theta_1)$是一个凸函数

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-24.png" alt="2-24" style="zoom:50%;" />

### 线性回归算法的梯度下降过程

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-25.png" alt="2-25" style="zoom:50%;" />

### Batch梯度下降算法

<img src="D:/【2】量化学习/【5】吴恩达机器学习/2/2-26.png" alt="2-26" style="zoom:50%;" />

Batch梯度下降算法在每一步都用了训练集内所有的examples，以后还会学习其他的梯度下降算法，只用一个子集







